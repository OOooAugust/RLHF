{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679972cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification, pipeline\n",
    "from datasets import load_dataset, Dataset\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from trl import DPOTrainer, DPOConfig, ModelConfig,get_quantization_config,get_kbit_device_map\n",
    "\n",
    "# Load environment variables from /etc/network_turbo\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "# Set the model path of qwen sft AND sentiment model\n",
    "LM_MODEL_AS = \"august66/qwen2-sft-final\"\n",
    "LM_MODEL_KY = 'Kyleyee/Qwen2-0.5B-stf-imdb'\n",
    "SENTIMENT_MODEL = \"siebert/sentiment-roberta-large-english\"\n",
    "N_PREFIX_TOKENS = 5\n",
    "\n",
    "\n",
    "#load dataset\n",
    "dataset_test = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
    "dataset_train = load_dataset(\"stanfordnlp/imdb\", split=\"train\")\n",
    "def prompt_completion_preprocess(example):\n",
    "    words = example['text'].split()\n",
    "    prompt = ' '.join(words[:N_PREFIX_TOKENS])\n",
    "    completion = ' '.join(words[N_PREFIX_TOKENS:])\n",
    "    return {'prompt': prompt, 'completion': completion}\n",
    "dataset_test = dataset_test.map(prompt_completion_preprocess, remove_columns=['text', 'label'])\n",
    "dataset_train = dataset_train.map(prompt_completion_preprocess, remove_columns=['text', 'label'])\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04bcbd5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "qwen_sft_model = AutoModelForCausalLM.from_pretrained(LM_MODEL_AS)\n",
    "qwen_sft_tokenizer = AutoTokenizer.from_pretrained(LM_MODEL_AS)\n",
    "qwen_sft_tokenizer.padding_side = \"left\"\n",
    "if qwen_sft_tokenizer.pad_token is None:\n",
    "    qwen_sft_tokenizer.pad_token = qwen_sft_tokenizer.eos_token\n",
    "pipe_qwen_sft = pipeline(\n",
    "    'text-generation',\n",
    "    model = qwen_sft_model,\n",
    "    tokenizer = qwen_sft_tokenizer,\n",
    "    device_map = 'auto'\n",
    ")\n",
    "prompts_train = dataset_train['prompt']\n",
    "generated_completions_train = pipe_qwen_sft(\n",
    "    prompts_train,\n",
    "    max_new_tokens = 128,\n",
    "    do_sample = True,\n",
    "    top_p = 0.95,\n",
    "    top_k = 50,\n",
    "    temperature = 1,\n",
    "    num_return_sequences = 2,\n",
    "    batch_size = 128,\n",
    "    repetition_penalty = 1.2,\n",
    "    eos_token_id = qwen_sft_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "pipe_qwen_sft.model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "generated_completions_train_flat = Dataset.from_list(list(np.array(generated_completions_train).ravel()))\n",
    "\n",
    "pipe_sentiment = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model = SENTIMENT_MODEL,\n",
    ")\n",
    "\n",
    "train_sentiment_results = pipe_sentiment(\n",
    "    generated_completions_train_flat['generated_text'],\n",
    "    batch_size = 128,\n",
    ")\n",
    "\n",
    "pipe_sentiment.model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "prompt_completion_list_train = []\n",
    "for i in range(25000):\n",
    "\n",
    "    prompt = dataset_train[i]['prompt']\n",
    "    completion_1 = generated_completions_train_flat[2*i]['generated_text']\n",
    "    reward_1 = train_sentiment_results[2*i]['score'] if train_sentiment_results[2*i]['label'] == 'POSITIVE' else 1-train_sentiment_results[2*i]['score']\n",
    "    completion_2 = generated_completions_train_flat[2*i + 1]['generated_text']\n",
    "    reward_2 = train_sentiment_results[2*i + 1]['score'] if train_sentiment_results[2*i + 1]['label'] == 'POSITIVE' else 1-train_sentiment_results[2*i + 1]['score']\n",
    "    preference_prob = F.sigmoid(torch.tensor(reward_1-reward_2))\n",
    "    bernoulli_indicator = torch.bernoulli(preference_prob).item()\n",
    "    if bernoulli_indicator == 1:\n",
    "        chosen, rejected = completion_1, completion_2\n",
    "        reward_chosen, reward_rejected = reward_1, reward_2\n",
    "    else:\n",
    "        chosen, rejected = completion_2, completion_1\n",
    "        reward_chosen, reward_rejected = reward_2, reward_1\n",
    "    prompt_completion_list_train.append({\n",
    "        'prompt': prompt,\n",
    "        'chosen': \" \".join(chosen.split()[N_PREFIX_TOKENS:]),\n",
    "        'rejected': \" \".join(rejected.split()[N_PREFIX_TOKENS:]),\n",
    "        'reward_chosen': reward_chosen,\n",
    "        'reward_rejected': reward_rejected\n",
    "    })\n",
    "    \n",
    "prompt_completion_dataset_train = Dataset.from_list(prompt_completion_list_train)\n",
    "dpo_dataset_train = prompt_completion_dataset_train.select_columns(['prompt', 'chosen', 'rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f94385a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_completion_list_train = []\n",
    "for i in range(25000):\n",
    "\n",
    "    prompt = dataset_train[i]['prompt']\n",
    "    completion_1 = generated_completions_train_flat[2*i]['generated_text']\n",
    "    reward_1 = train_sentiment_results[2*i]['score'] if train_sentiment_results[2*i]['label'] == 'POSITIVE' else 1-train_sentiment_results[2*i]['score']\n",
    "    completion_2 = generated_completions_train_flat[2*i + 1]['generated_text']\n",
    "    reward_2 = train_sentiment_results[2*i + 1]['score'] if train_sentiment_results[2*i + 1]['label'] == 'POSITIVE' else 1-train_sentiment_results[2*i + 1]['score']\n",
    "    preference_prob = F.sigmoid(torch.tensor(reward_1-reward_2))\n",
    "    bernoulli_indicator = torch.bernoulli(preference_prob).item()\n",
    "    if bernoulli_indicator == 1:\n",
    "        chosen, rejected = completion_1, completion_2\n",
    "        reward_chosen, reward_rejected = reward_1, reward_2\n",
    "    else:\n",
    "        chosen, rejected = completion_2, completion_1\n",
    "        reward_chosen, reward_rejected = reward_2, reward_1\n",
    "    prompt_completion_list_train.append({\n",
    "        'prompt': prompt,\n",
    "        'chosen': \" \".join(chosen.split()[N_PREFIX_TOKENS:]),\n",
    "        'rejected': \" \".join(rejected.split()[N_PREFIX_TOKENS:]),\n",
    "        'reward_chosen': reward_chosen,\n",
    "        'reward_rejected': reward_rejected\n",
    "    })\n",
    "    \n",
    "prompt_completion_dataset_train = Dataset.from_list(prompt_completion_list_train)\n",
    "dpo_dataset_train = prompt_completion_dataset_train.select_columns(['prompt', 'chosen', 'rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "00655f84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b76c16a414b4262a3ad91e643153e36",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Uploading the dataset shards:   0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5513c6b3fd634a8db3c05296a0be159e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating parquet from Arrow format:   0%|          | 0/25 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "473d0046f0f54350a0cc5ef567edfa3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/440 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/datasets/august66/reward_data_for_dpo_train/commit/f7353cc4aaf6c0490acc9908788306e34a0ca319', commit_message='Upload dataset', commit_description='', oid='f7353cc4aaf6c0490acc9908788306e34a0ca319', pr_url=None, repo_url=RepoUrl('https://huggingface.co/datasets/august66/reward_data_for_dpo_train', endpoint='https://huggingface.co', repo_type='dataset', repo_id='august66/reward_data_for_dpo_train'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_completion_dataset_train.push_to_hub(\"august66/reward_data_for_dpo_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6ef420e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b40cdd7973bf4b37ab9d439fa9412354",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/721 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "942ddf95e1944d66859822cf59411158",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72bcb5ac596a45a6ade0659a081fd13d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fecd1b74b8c34072b948ec7554be4ec3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20f5b6a717b8424aaaf2d8179a7e2855",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fc07ca12d2d41bfbf61b4ecedc6c6a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8251aa4a446b482282cb26eedba1a041",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67f86f827d24e1a95fea8e960425f2a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1f214936a334894b39e7802c94c7f5b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "qwen_sft_model = AutoModelForCausalLM.from_pretrained(LM_MODEL_KY)\n",
    "qwen_sft_tokenizer = AutoTokenizer.from_pretrained(LM_MODEL_KY)\n",
    "qwen_sft_tokenizer.padding_side = \"left\"\n",
    "if qwen_sft_tokenizer.pad_token is None:\n",
    "    qwen_sft_tokenizer.pad_token = qwen_sft_tokenizer.eos_token\n",
    "pipe_qwen_sft = pipeline(\n",
    "    'text-generation',\n",
    "    model = qwen_sft_model,\n",
    "    tokenizer = qwen_sft_tokenizer,\n",
    "    device_map = 'auto'\n",
    ")\n",
    "prompts_train = dataset_train['prompt']\n",
    "generated_completions_train = pipe_qwen_sft(\n",
    "    prompts_train,\n",
    "    max_new_tokens = 128,\n",
    "    do_sample = True,\n",
    "    top_p = 0.95,\n",
    "    top_k = 50,\n",
    "    temperature = 1,\n",
    "    num_return_sequences = 2,\n",
    "    batch_size = 128,\n",
    "    repetition_penalty = 1.2,\n",
    "    eos_token_id = qwen_sft_tokenizer.eos_token_id,\n",
    ")\n",
    "\n",
    "\n",
    "pipe_qwen_sft.model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "generated_completions_train_flat = Dataset.from_list(list(np.array(generated_completions_train).ravel()))\n",
    "\n",
    "pipe_sentiment = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model = SENTIMENT_MODEL,\n",
    ")\n",
    "\n",
    "train_sentiment_results = pipe_sentiment(\n",
    "    generated_completions_train_flat['generated_text'],\n",
    "    batch_size = 128,\n",
    ")\n",
    "\n",
    "pipe_sentiment.model.to(\"cpu\")\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "prompt_completion_list_train = []\n",
    "for i in range(N):\n",
    "\n",
    "    prompt = dataset_train[i]['prompt']\n",
    "    completion_1 = generated_completions_train_flat[2*i]['generated_text']\n",
    "    reward_1 = train_sentiment_results[2*i]['score'] if train_sentiment_results[2*i]['score'] == 'POSITIVE' else 1-train_sentiment_results[2*i]['score']\n",
    "    completion_2 = generated_completions_train_flat[2*i + 1]['generated_text']\n",
    "    reward_2 = train_sentiment_results[2*i + 1]['score'] if train_sentiment_results[2*i + 1]['score'] == 'POSITIVE' else 1-train_sentiment_results[2*i + 1]['score']\n",
    "    preference_prob = F.sigmoid(torch.tensor(reward_1-reward_2))\n",
    "    bernoulli_indicator = torch.bernoulli(preference_prob).item()\n",
    "    if bernoulli_indicator == 1:\n",
    "        chosen, rejected = completion_1, completion_2\n",
    "        reward_chosen, reward_rejected = reward_1, reward_2\n",
    "    else:\n",
    "        chosen, rejected = completion_2, completion_1\n",
    "        reward_chosen, reward_rejected = reward_2, reward_1\n",
    "    prompt_completion_list_train.append({\n",
    "        'prompt': prompt,\n",
    "        'chosen': \" \".join(chosen.split()[N_PREFIX_TOKENS:]),\n",
    "        'rejected': \" \".join(rejected.split()[N_PREFIX_TOKENS:]),\n",
    "        'reward_chosen': reward_chosen,\n",
    "        'reward_rejected': reward_rejected\n",
    "    })\n",
    "    \n",
    "prompt_completion_dataset_train = Dataset.from_list(prompt_completion_list_train)\n",
    "dpo_dataset_train = prompt_completion_dataset_train.select_columns(['prompt', 'chosen', 'rejected'])\n",
    "\n",
    "qwen_sft_model_ky = qwen_sft_model.to('cpu')\n",
    "prompt_completion_dataset_train_ky = prompt_completion_dataset_train\n",
    "train_sentiment_results_ky = train_sentiment_results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d9a74bbf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_sentiment_results_ky' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m      2\u001b[0m ds_train_sentiment_results_as \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(train_sentiment_results_as)\n\u001b[0;32m----> 3\u001b[0m ds_train_sentiment_results_ky \u001b[38;5;241m=\u001b[39m Dataset\u001b[38;5;241m.\u001b[39mfrom_list(\u001b[43mtrain_sentiment_results_ky\u001b[49m)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# 1) Pull out the “reward_chosen” column into a plain list (or np.array)\u001b[39;00m\n\u001b[1;32m      5\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreward_chosen\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_sentiment_results_ky' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "ds_train_sentiment_results_as = Dataset.from_list(train_sentiment_results_as)\n",
    "ds_train_sentiment_results_ky = Dataset.from_list(train_sentiment_results_ky)\n",
    "# 1) Pull out the “reward_chosen” column into a plain list (or np.array)\n",
    "name = 'reward_chosen'\n",
    "rewards_ky = prompt_completion_dataset_train_ky[name]\n",
    "\n",
    "# 2) Wrap it in a DataFrame and call .describe()\n",
    "df_reward_ky = pd.DataFrame({name: rewards_ky})\n",
    "print(df_reward_ky[name].describe())\n",
    "\n",
    "# 1) Pull out the “reward_chosen” column into a plain list (or np.array)\n",
    "rewards_as = prompt_completion_dataset_train_as[name]\n",
    "\n",
    "# 2) Wrap it in a DataFrame and call .describe()\n",
    "df_reward_as = pd.DataFrame({name: rewards_as})\n",
    "print(df_reward_as[name].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7a3eec16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AS‐styled sentiment distribution:\n",
      "'NEGATIVE': 2459 / 4000 → 61.48%\n",
      "'POSITIVE': 1541 / 4000 → 38.52%\n",
      "\n",
      "KY‐styled sentiment distribution:\n",
      "'NEGATIVE': 2470 / 4000 → 61.75%\n",
      "'POSITIVE': 1530 / 4000 → 38.25%\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def label_percentages(ds):\n",
    "    # 1) Pull out the entire 'label' column as a Python list\n",
    "    labels = ds[\"label\"]  # e.g. [\"positive\", \"negative\", \"positive\", ...]\n",
    "    total = len(labels)\n",
    "\n",
    "    # 2) Count how many times each label occurs\n",
    "    counts = Counter(labels)\n",
    "\n",
    "    # 3) Compute percentages\n",
    "    for label, count in counts.items():\n",
    "        pct = count / total * 100\n",
    "        print(f\"{label!r}: {count} / {total} → {pct:.2f}%\")\n",
    "\n",
    "# Example usage:\n",
    "print(\"AS‐styled sentiment distribution:\")\n",
    "label_percentages(ds_train_sentiment_results_as)\n",
    "\n",
    "print(\"\\nKY‐styled sentiment distribution:\")\n",
    "label_percentages(ds_train_sentiment_results_ky)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1aae1297",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e022650e7b6d47cf94388e09efd1af43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Extracting prompt in train dataset:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2280e961bbf644018450dd29a2f59de7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train dataset:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e3a6d6c9089b47859cc0df8c0f864031",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3125' max='3125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3125/3125 25:46, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>0.693700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.691700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.687200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.685900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.688700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.696600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.687800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.680100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>450</td>\n",
       "      <td>0.672500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.693800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>550</td>\n",
       "      <td>0.693100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>600</td>\n",
       "      <td>0.678900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>650</td>\n",
       "      <td>0.690700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>700</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>750</td>\n",
       "      <td>0.694400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>800</td>\n",
       "      <td>0.705000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>850</td>\n",
       "      <td>0.678700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>900</td>\n",
       "      <td>0.685000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>950</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.689400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1050</td>\n",
       "      <td>0.688000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1100</td>\n",
       "      <td>0.674000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1150</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1200</td>\n",
       "      <td>0.684300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1250</td>\n",
       "      <td>0.705600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1300</td>\n",
       "      <td>0.680000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1350</td>\n",
       "      <td>0.681800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1400</td>\n",
       "      <td>0.669600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1450</td>\n",
       "      <td>0.692300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.668600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1550</td>\n",
       "      <td>0.685400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1600</td>\n",
       "      <td>0.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1650</td>\n",
       "      <td>0.687600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1700</td>\n",
       "      <td>0.681200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1750</td>\n",
       "      <td>0.682600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1800</td>\n",
       "      <td>0.673300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1850</td>\n",
       "      <td>0.671700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1900</td>\n",
       "      <td>0.686900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1950</td>\n",
       "      <td>0.674100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2050</td>\n",
       "      <td>0.697200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2100</td>\n",
       "      <td>0.665000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2150</td>\n",
       "      <td>0.690800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2200</td>\n",
       "      <td>0.671900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2250</td>\n",
       "      <td>0.692500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2300</td>\n",
       "      <td>0.671500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2350</td>\n",
       "      <td>0.679000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2400</td>\n",
       "      <td>0.674400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2450</td>\n",
       "      <td>0.708900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.668400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2550</td>\n",
       "      <td>0.689500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2600</td>\n",
       "      <td>0.694800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2650</td>\n",
       "      <td>0.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2700</td>\n",
       "      <td>0.685800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2750</td>\n",
       "      <td>0.688500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2800</td>\n",
       "      <td>0.680900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2850</td>\n",
       "      <td>0.682000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2900</td>\n",
       "      <td>0.678000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2950</td>\n",
       "      <td>0.689600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.672100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3050</td>\n",
       "      <td>0.692400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3100</td>\n",
       "      <td>0.655100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3125, training_loss=0.6838850622558593, metrics={'train_runtime': 1546.7332, 'train_samples_per_second': 16.163, 'train_steps_per_second': 2.02, 'total_flos': 0.0, 'train_loss': 0.6838850622558593, 'epoch': 1.0})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#why random sample?\n",
    "#what is gradient checking, gradient acc, learning rate \n",
    "torch.cuda.empty_cache()\n",
    "model_args = ModelConfig(LM_MODEL_AS)\n",
    "beta = 0.1 \n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype if model_args.torch_dtype in ['auto', None] else torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = torch_dtype,\n",
    "    attn_implementation = model_args.attn_implementation,\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs\n",
    ") \n",
    "ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    **model_kwargs,\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    padding_side = \"left\",\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "\n",
    "training_args = DPOConfig(\n",
    "\n",
    "        gradient_checkpointing=False,\n",
    "        per_device_train_batch_size=4,\n",
    "        gradient_accumulation_steps=2,\n",
    "        learning_rate=5.0e-7,\n",
    "        logging_steps=50,\n",
    "        num_train_epochs=1,\n",
    "        push_to_hub=True,  \n",
    "        output_dir = \"/root/autodl-tmp/.autodl/DPO_tldr\",\n",
    "        report_to = 'none',\n",
    "        beta = beta,\n",
    "        hub_model_id = f'august66/qwen2-sft-dpo-imdb-beta-{beta}',\n",
    "        save_strategy=\"no\", \n",
    "    )\n",
    "\n",
    "trainer = DPOTrainer(\n",
    "    model=model,\n",
    "    ref_model=ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dpo_dataset_train,\n",
    "    processing_class = tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e233104f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "prompts_test = dataset_test['prompt']\n",
    "dpo_model = trainer.model\n",
    "dpo_tokenizer = trainer.processing_class\n",
    "\n",
    "dpo_pipe = pipeline(\n",
    "    'text-generation',\n",
    "    model = dpo_model,\n",
    "    tokenizer = dpo_tokenizer,\n",
    ")\n",
    "dpo_completions_test = dpo_pipe(\n",
    "    prompts_test,\n",
    "    max_new_tokens = 128,\n",
    "    eos_token_id = dpo_tokenizer.eos_token_id,\n",
    "    return_full_text = False,\n",
    "    batch_size = 128,\n",
    "    temperature = 1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "1e84aa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "dpo_completion_test_flat= Dataset.from_list(list(np.array(dpo_completions_test).ravel()))\n",
    "pipe = pipeline(\n",
    "    'sentiment-analysis',\n",
    "    model = SENTIMENT_MODEL,\n",
    ")\n",
    "\n",
    "dpo_sentiment_analysis_test = pipe(\n",
    "    dpo_completion_test_flat['generated_text'],\n",
    "    batch_size = 128,\n",
    "    truncation = True,\n",
    "    padding = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "2282d6d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_score = 0\n",
    "for i in range(len(dpo_sentiment_analysis_test)):\n",
    "    score = dpo_sentiment_analysis_test[i]['score']\n",
    "    if dpo_sentiment_analysis_test[i]['label'] == 'NEGATIVE':\n",
    "        score = 1 - score\n",
    "    total_score += score\n",
    "average_score = total_score / len(dpo_sentiment_analysis_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d8813cbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8575751939988137"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "76183ac6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d0b50f7c55045f78bfe428fc7dd67eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/720 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4e81524be984682badae7f67636db3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/1.98G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "84a97d792a1543478915cac2b4006a95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/117 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "897f632c373645cdaeb384a74e8981a4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.33k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ac5a81e358c74b068e41c570b8959840",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ccb7881ff914bf4b2c535e74d95e287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d30853940ea1457b8ae2a1021f745cfb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/11.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d6156222eda24eb8892567a64bbcc255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/80.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad09f82c9fe145a79e0fc05246e5d7ae",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/370 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "KY_DPO_MODEL = \"Kyleyee/Qwen2-0.5B-DPO-imdb-bm\"\n",
    "model_args = ModelConfig(KY_DPO_MODEL)\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype if model_args.torch_dtype in ['auto', None] else torch.float16\n",
    ")\n",
    "\n",
    "\n",
    "model_kwargs = dict(\n",
    "    revision = model_args.model_revision,\n",
    "    torch_dtype = torch_dtype,\n",
    "    attn_implementation = model_args.attn_implementation,\n",
    "    trust_remote_code = model_args.trust_remote_code,\n",
    ")\n",
    "\n",
    "\n",
    "dpo_model_ky = AutoModelForCausalLM.from_pretrained(\n",
    "    KY_DPO_MODEL,\n",
    "    torch_dtype=torch_dtype,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    "    trust_remote_code=model_args.trust_remote_code\n",
    ")\n",
    "dpo_tokenizer_ky = AutoTokenizer.from_pretrained(\n",
    "    KY_DPO_MODEL,\n",
    "    padding_side=\"left\",\n",
    "    trust_remote_code=model_args.trust_remote_code\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e77a65c7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4e52dc55f26a4491a1029ec5cef22b86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/684 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "❗ Found 3 differing config fields:\n",
      "\n",
      "• '_name_or_path':\n",
      "    – Kyleyee/Qwen2-0.5B-DPO-imdb-bm: \"Kyleyee/Qwen2-0.5B-DPO-imdb-bm\"\n",
      "    – august66/qwen2-sft-dpo-imdb-beta-0.1: \"august66/qwen2-sft-dpo-imdb-beta-0.1\"\n",
      "\n",
      "• 'sliding_window':\n",
      "    – Kyleyee/Qwen2-0.5B-DPO-imdb-bm: null\n",
      "    – august66/qwen2-sft-dpo-imdb-beta-0.1: 131072\n",
      "\n",
      "• 'use_cache':\n",
      "    – Kyleyee/Qwen2-0.5B-DPO-imdb-bm: true\n",
      "    – august66/qwen2-sft-dpo-imdb-beta-0.1: null\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoConfig\n",
    "import json\n",
    "\n",
    "def compare_model_configs(model_id_1, model_id_2):\n",
    "    # 1) Load each model’s config\n",
    "    cfg1 = AutoConfig.from_pretrained(model_id_1)\n",
    "    cfg2 = AutoConfig.from_pretrained(model_id_2)\n",
    "\n",
    "    # 2) Convert to plain dicts\n",
    "    dict1 = cfg1.to_dict()\n",
    "    dict2 = cfg2.to_dict()\n",
    "\n",
    "    # 3) Collect all keys\n",
    "    all_keys = set(dict1.keys()) | set(dict2.keys())\n",
    "\n",
    "    # 4) Print any fields whose values differ\n",
    "    diffs = []\n",
    "    for key in sorted(all_keys):\n",
    "        v1 = dict1.get(key, \"<missing>\")\n",
    "        v2 = dict2.get(key, \"<missing>\")\n",
    "        if v1 != v2:\n",
    "            diffs.append((key, v1, v2))\n",
    "\n",
    "    if not diffs:\n",
    "        print(\"✅ No configuration differences found.\")\n",
    "        return\n",
    "\n",
    "    print(f\"❗ Found {len(diffs)} differing config fields:\\n\")\n",
    "    for key, v1, v2 in diffs:\n",
    "        print(f\"• {key!r}:\")\n",
    "        print(f\"    – {model_id_1}: {json.dumps(v1, default=str)}\")\n",
    "        print(f\"    – {model_id_2}: {json.dumps(v2, default=str)}\\n\")\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "model_1 = KY_DPO_MODEL\n",
    "model_2 = AS_DPO_MODEL\n",
    "compare_model_configs(model_1, model_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9303662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top‐10 parameters by L2 difference:\n",
      "model.embed_tokens.weight                                    → L2 diff = 8.046e+00\n",
      "lm_head.weight                                               → L2 diff = 8.046e+00\n",
      "model.layers.9.mlp.down_proj.weight                          → L2 diff = 7.404e-01\n",
      "model.layers.10.mlp.down_proj.weight                         → L2 diff = 7.352e-01\n",
      "model.layers.8.mlp.down_proj.weight                          → L2 diff = 7.243e-01\n",
      "model.layers.7.mlp.down_proj.weight                          → L2 diff = 7.234e-01\n",
      "model.layers.11.mlp.down_proj.weight                         → L2 diff = 7.221e-01\n",
      "model.layers.10.mlp.up_proj.weight                           → L2 diff = 7.179e-01\n",
      "model.layers.20.mlp.down_proj.weight                         → L2 diff = 7.163e-01\n",
      "model.layers.10.mlp.gate_proj.weight                         → L2 diff = 7.155e-01\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model_id_a = KY_DPO_MODEL\n",
    "model_id_b = AS_DPO_MODEL\n",
    "\n",
    "# Load both models on CPU to avoid OOM:\n",
    "model_a = AutoModelForCausalLM.from_pretrained(model_id_a, device_map=None, torch_dtype=torch.float32).to(\"cpu\")\n",
    "model_b = AutoModelForCausalLM.from_pretrained(model_id_b, device_map=None, torch_dtype=torch.float32).to(\"cpu\")\n",
    "\n",
    "sd_a = model_a.state_dict()\n",
    "sd_b = model_b.state_dict()\n",
    "\n",
    "# Find keys that exist in both:\n",
    "common_keys = [k for k in sd_a.keys() if k in sd_b.keys()]\n",
    "\n",
    "# Compute per‐parameter difference norms (L2)\n",
    "diff_norms = []\n",
    "for key in common_keys:\n",
    "    w1 = sd_a[key]\n",
    "    w2 = sd_b[key]\n",
    "    if w1.shape != w2.shape:\n",
    "        # e.g. fine‐tuned model might have extra heads or adapter layers\n",
    "        continue\n",
    "    diff = (w1 - w2).float()  # cast to float32 for safety\n",
    "    diff_norm = diff.norm().item()  # L2 norm\n",
    "    diff_norms.append((key, diff_norm))\n",
    "\n",
    "# Sort by descending difference\n",
    "diff_norms.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Print out top 10 parameters whose weights changed most\n",
    "print(\"Top‐10 parameters by L2 difference:\")\n",
    "for key, norm_val in diff_norms[:10]:\n",
    "    print(f\"{key:60s} → L2 diff = {norm_val:.3e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5ed796",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

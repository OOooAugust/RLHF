{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "679972cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Allocated: 0.01 GB\n",
      "Reserved : 1.75 GB\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "from datasets import load_dataset, Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "\n",
    "LM_MODEL = \"august66/qwen2-sft-final\"\n",
    "SENTIMENT_MODEL = \"siebert/sentiment-roberta-large-english\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "sft_model = AutoModelForCausalLM.from_pretrained(\n",
    "    LM_MODEL,\n",
    "    torch_dtype = torch.float16\n",
    ")\n",
    "sft_tokenizer = AutoTokenizer.from_pretrained(\"august66/qwen2-sft-final\") \n",
    "sft_tokenizer.pad_token = sft_tokenizer.eos_token\n",
    "sft_model.config.pad_token_id = sft_model.config.eos_token_id\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(SENTIMENT_MODEL)\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(SENTIMENT_MODEL)\n",
    "\n",
    "dataset_test = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
    "def prompt_completion_preprocess(example):\n",
    "    words = example['text'].split()\n",
    "    prompt = ' '.join(words[:5])\n",
    "    completion = ' '.join(words[5:])\n",
    "    return {'prompt': prompt, 'completion': completion}\n",
    "dataset_test = dataset_test.map(prompt_completion_preprocess, remove_columns=['text', 'label'])\n",
    "\n",
    "\n",
    "print(f\"Allocated: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
    "print(f\"Reserved : {torch.cuda.memory_reserved()  / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "392a79a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 782/782 [03:22<00:00,  3.86it/s]\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "def tokenize(batch):\n",
    "\n",
    "    inputs = sft_tokenizer(\n",
    "        batch['prompt'],\n",
    "        padding = 'max_length',\n",
    "        truncation = True,\n",
    "        max_length = 128,\n",
    "        padding_side = 'left',\n",
    "        add_special_tokens = True, \n",
    "        return_tensors = 'pt'\n",
    "    )\n",
    "\n",
    "    return {k:v for k, v in inputs.items()}\n",
    "\n",
    "tokenized_inputs = dataset_test.map(\n",
    "    tokenize, \n",
    "    batched = True,\n",
    "    batch_size = 32, \n",
    "    remove_columns = ['prompt', 'completion']\n",
    ").with_format('torch', columns = ['input_ids', 'attention_mask'])\n",
    "\n",
    "\n",
    "tokenized_inputs_dataloader = DataLoader(\n",
    "    tokenized_inputs,\n",
    "    batch_size = 32, \n",
    "    shuffle = True,\n",
    "    pin_memory = True \n",
    ")\n",
    "\n",
    "decoded_outputs = []\n",
    "with torch.inference_mode():\n",
    "    sft_model = sft_model.to(device)\n",
    "    for input_batch in tqdm(tokenized_inputs_dataloader):\n",
    "        input_batch = {k:v.to(device) for k, v in input_batch.items()}\n",
    "        batch_size = input_batch['input_ids'].shape[0]\n",
    "\n",
    "        outputs = sft_model.generate(\n",
    "            **input_batch,\n",
    "            num_return_sequences = 2, \n",
    "            do_sample = True,\n",
    "            top_k=50,\n",
    "            top_p=0.95,\n",
    "            temperature=1.0,\n",
    "            use_cache = True,\n",
    "            pad_token_id = sft_model.config.eos_token_id,\n",
    "            max_new_tokens=10\n",
    "        ).view(batch_size * 2, -1).cpu()\n",
    "        \n",
    "        decoded_outputs_batch = sft_tokenizer.batch_decode(\n",
    "            outputs,\n",
    "            skip_special_tokens = True\n",
    "        )\n",
    "\n",
    "        decoded_outputs.extend(decoded_outputs_batch)\n",
    "\n",
    "prompt_completion_data = [\n",
    "    {\n",
    "        'prompt':prompt,\n",
    "        'completion_1':decoded_outputs[2*index],\n",
    "        'completion_2':decoded_outputs[2*index+1]\n",
    "    }\n",
    "    for index, prompt in enumerate(dataset_test['prompt'])\n",
    "]\n",
    "\n",
    "prompt_completion_dataset = Dataset.from_list(prompt_completion_data)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6be44ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_dataset = prompt_completion_dataset.select_columns(['completion_1', 'completion_2'])\n",
    "completions_dataset_flat = [x for row in completions_dataset for x in row]\n",
    "tokenized_completion = sentiment_tokenizer(\n",
    "    text = completions_dataset_flat,\n",
    "    max_length = 128,\n",
    "    add_special_tokens = True,\n",
    "    return_tensors = 'pt',\n",
    "    padding = True, \n",
    "    truncation = True,\n",
    "    padding_side = 'left'\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c0f971",
   "metadata": {},
   "outputs": [],
   "source": [
    "completions_arr = np.array(prompt_completion_data.select_columns(['completion_1', 'completion_2']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e06d2bd0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'completion_1': 'Sure, this film was retarded. A retarded film. It was so retarded that I', 'completion_2': \"Sure, this film was retarded. It wasn't very original. I mean... I\"},\n",
       "       {'completion_1': 'This review is based on the Japanese Region 1 DVD version only. The', 'completion_2': 'This review is based on the novel by Richard Condon.<br /><br'},\n",
       "       {'completion_1': 'Viva Variety was a unique production. You have the occasional movie from the mid', 'completion_2': 'Viva Variety was a unique program. It showcased the best of the local and'},\n",
       "       ...,\n",
       "       {'completion_1': 'How to qualify this film, as \"worst movie ever made\" I can', 'completion_2': 'How to qualify this film, you would have to have been involved in a cinematic'},\n",
       "       {'completion_1': 'Imagine every stereotypical, overacted cliche about the Mafia or the underbelly of American', 'completion_2': 'Imagine every stereotypical, overacted cliche about Hitler that comes out every year (especially from'},\n",
       "       {'completion_1': \"Oz is a great series, as long as you don't ask yourself the following\", 'completion_2': 'Oz is a great series, that will have you hooked! I agree with everyone'}],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81013463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('positive', [0.0011193063110113144, 0.9988806843757629])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_sentiment(text: str):\n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = sentiment_model(**inputs).logits\n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)[0]\n",
    "    # Decode prediction\n",
    "    label = \"positive\" if probs[1] > probs[0] else \"negative\"\n",
    "    return label, probs.cpu().tolist()\n",
    "\n",
    "# ðŸ” Example:\n",
    "print(get_sentiment(completions[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "7a3e711c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0;31mSignature:\u001b[0m     \n",
      "\u001b[0msentiment_tokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtext_pair_target\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0madd_special_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPaddingStrategy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mtruncation\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTruncationStrategy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mstride\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mint\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mis_split_into_words\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpad_to_multiple_of\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mpadding_side\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_tensors\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mUnion\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorType\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNoneType\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_token_type_ids\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_attention_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbool\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_overflowing_tokens\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_special_tokens_mask\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_offsets_mapping\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mreturn_length\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mbool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m    \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\n",
      "\u001b[0;34m\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mtransformers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenization_utils_base\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBatchEncoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mType:\u001b[0m           RobertaTokenizerFast\n",
      "\u001b[0;31mString form:\u001b[0m   \n",
      "RobertaTokenizerFast(name_or_path='siebert/sentiment-roberta-large-english', vocab_size=50265, mo <...> oken(\"<mask>\", rstrip=False, lstrip=True, single_word=False, normalized=False, special=True),\n",
      "           }\n",
      "           )\n",
      "\u001b[0;31mLength:\u001b[0m         50265\n",
      "\u001b[0;31mFile:\u001b[0m           ~/miniconda3/lib/python3.12/site-packages/transformers/models/roberta/tokenization_roberta_fast.py\n",
      "\u001b[0;31mDocstring:\u001b[0m     \n",
      "Construct a \"fast\" RoBERTa tokenizer (backed by HuggingFace's *tokenizers* library), derived from the GPT-2\n",
      "tokenizer, using byte-level Byte-Pair-Encoding.\n",
      "\n",
      "This tokenizer has been trained to treat spaces like parts of the tokens (a bit like sentencepiece) so a word will\n",
      "be encoded differently whether it is at the beginning of the sentence (without space) or not:\n",
      "\n",
      "```python\n",
      ">>> from transformers import RobertaTokenizerFast\n",
      "\n",
      ">>> tokenizer = RobertaTokenizerFast.from_pretrained(\"FacebookAI/roberta-base\")\n",
      ">>> tokenizer(\"Hello world\")[\"input_ids\"]\n",
      "[0, 31414, 232, 2]\n",
      "\n",
      ">>> tokenizer(\" Hello world\")[\"input_ids\"]\n",
      "[0, 20920, 232, 2]\n",
      "```\n",
      "\n",
      "You can get around that behavior by passing `add_prefix_space=True` when instantiating this tokenizer or when you\n",
      "call it on some text, but since the model was not pretrained this way, it might yield a decrease in performance.\n",
      "\n",
      "<Tip>\n",
      "\n",
      "When used with `is_split_into_words=True`, this tokenizer needs to be instantiated with `add_prefix_space=True`.\n",
      "\n",
      "</Tip>\n",
      "\n",
      "This tokenizer inherits from [`PreTrainedTokenizerFast`] which contains most of the main methods. Users should\n",
      "refer to this superclass for more information regarding those methods.\n",
      "\n",
      "Args:\n",
      "    vocab_file (`str`):\n",
      "        Path to the vocabulary file.\n",
      "    merges_file (`str`):\n",
      "        Path to the merges file.\n",
      "    errors (`str`, *optional*, defaults to `\"replace\"`):\n",
      "        Paradigm to follow when decoding bytes to UTF-8. See\n",
      "        [bytes.decode](https://docs.python.org/3/library/stdtypes.html#bytes.decode) for more information.\n",
      "    bos_token (`str`, *optional*, defaults to `\"<s>\"`):\n",
      "        The beginning of sequence token that was used during pretraining. Can be used a sequence classifier token.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        When building a sequence using special tokens, this is not the token that is used for the beginning of\n",
      "        sequence. The token used is the `cls_token`.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    eos_token (`str`, *optional*, defaults to `\"</s>\"`):\n",
      "        The end of sequence token.\n",
      "\n",
      "        <Tip>\n",
      "\n",
      "        When building a sequence using special tokens, this is not the token that is used for the end of sequence.\n",
      "        The token used is the `sep_token`.\n",
      "\n",
      "        </Tip>\n",
      "\n",
      "    sep_token (`str`, *optional*, defaults to `\"</s>\"`):\n",
      "        The separator token, which is used when building a sequence from multiple sequences, e.g. two sequences for\n",
      "        sequence classification or for a text and a question for question answering. It is also used as the last\n",
      "        token of a sequence built with special tokens.\n",
      "    cls_token (`str`, *optional*, defaults to `\"<s>\"`):\n",
      "        The classifier token which is used when doing sequence classification (classification of the whole sequence\n",
      "        instead of per-token classification). It is the first token of the sequence when built with special tokens.\n",
      "    unk_token (`str`, *optional*, defaults to `\"<unk>\"`):\n",
      "        The unknown token. A token that is not in the vocabulary cannot be converted to an ID and is set to be this\n",
      "        token instead.\n",
      "    pad_token (`str`, *optional*, defaults to `\"<pad>\"`):\n",
      "        The token used for padding, for example when batching sequences of different lengths.\n",
      "    mask_token (`str`, *optional*, defaults to `\"<mask>\"`):\n",
      "        The token used for masking values. This is the token used when training this model with masked language\n",
      "        modeling. This is the token which the model will try to predict.\n",
      "    add_prefix_space (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to add an initial space to the input. This allows to treat the leading word just as any\n",
      "        other word. (RoBERTa tokenizer detect beginning of words by the preceding space).\n",
      "    trim_offsets (`bool`, *optional*, defaults to `True`):\n",
      "        Whether the post processing step should trim offsets to avoid including whitespaces.\n",
      "\u001b[0;31mCall docstring:\u001b[0m\n",
      "Main method to tokenize and prepare for the model one or several sequence(s) or one or several pair(s) of\n",
      "sequences.\n",
      "\n",
      "Args:\n",
      "    text (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded. Each sequence can be a string or a list of strings\n",
      "        (pretokenized string). If the sequences are provided as list of strings (pretokenized), you must set\n",
      "        `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "    text_pair_target (`str`, `List[str]`, `List[List[str]]`, *optional*):\n",
      "        The sequence or batch of sequences to be encoded as target texts. Each sequence can be a string or a\n",
      "        list of strings (pretokenized string). If the sequences are provided as list of strings (pretokenized),\n",
      "        you must set `is_split_into_words=True` (to lift the ambiguity with a batch of sequences).\n",
      "\n",
      "    add_special_tokens (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to add special tokens when encoding the sequences. This will use the underlying\n",
      "        `PretrainedTokenizerBase.build_inputs_with_special_tokens` function, which defines which tokens are\n",
      "        automatically added to the input ids. This is useful if you want to add `bos` or `eos` tokens\n",
      "        automatically.\n",
      "    padding (`bool`, `str` or [`~utils.PaddingStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls padding. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest'`: Pad to the longest sequence in the batch (or no padding if only a single\n",
      "          sequence if provided).\n",
      "        - `'max_length'`: Pad to a maximum length specified with the argument `max_length` or to the maximum\n",
      "          acceptable input length for the model if that argument is not provided.\n",
      "        - `False` or `'do_not_pad'` (default): No padding (i.e., can output a batch with sequences of different\n",
      "          lengths).\n",
      "    truncation (`bool`, `str` or [`~tokenization_utils_base.TruncationStrategy`], *optional*, defaults to `False`):\n",
      "        Activates and controls truncation. Accepts the following values:\n",
      "\n",
      "        - `True` or `'longest_first'`: Truncate to a maximum length specified with the argument `max_length` or\n",
      "          to the maximum acceptable input length for the model if that argument is not provided. This will\n",
      "          truncate token by token, removing a token from the longest sequence in the pair if a pair of\n",
      "          sequences (or a batch of pairs) is provided.\n",
      "        - `'only_first'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the first sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `'only_second'`: Truncate to a maximum length specified with the argument `max_length` or to the\n",
      "          maximum acceptable input length for the model if that argument is not provided. This will only\n",
      "          truncate the second sequence of a pair if a pair of sequences (or a batch of pairs) is provided.\n",
      "        - `False` or `'do_not_truncate'` (default): No truncation (i.e., can output batch with sequence lengths\n",
      "          greater than the model maximum admissible input size).\n",
      "    max_length (`int`, *optional*):\n",
      "        Controls the maximum length to use by one of the truncation/padding parameters.\n",
      "\n",
      "        If left unset or set to `None`, this will use the predefined model maximum length if a maximum length\n",
      "        is required by one of the truncation/padding parameters. If the model has no specific maximum input\n",
      "        length (like XLNet) truncation/padding to a maximum length will be deactivated.\n",
      "    stride (`int`, *optional*, defaults to 0):\n",
      "        If set to a number along with `max_length`, the overflowing tokens returned when\n",
      "        `return_overflowing_tokens=True` will contain some tokens from the end of the truncated sequence\n",
      "        returned to provide some overlap between truncated and overflowing sequences. The value of this\n",
      "        argument defines the number of overlapping tokens.\n",
      "    is_split_into_words (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not the input is already pre-tokenized (e.g., split into words). If set to `True`, the\n",
      "        tokenizer assumes the input is already split into words (for instance, by splitting it on whitespace)\n",
      "        which it will tokenize. This is useful for NER or token classification.\n",
      "    pad_to_multiple_of (`int`, *optional*):\n",
      "        If set will pad the sequence to a multiple of the provided value. Requires `padding` to be activated.\n",
      "        This is especially useful to enable the use of Tensor Cores on NVIDIA hardware with compute capability\n",
      "        `>= 7.5` (Volta).\n",
      "    padding_side (`str`, *optional*):\n",
      "        The side on which the model should have padding applied. Should be selected between ['right', 'left'].\n",
      "        Default value is picked from the class attribute of the same name.\n",
      "    return_tensors (`str` or [`~utils.TensorType`], *optional*):\n",
      "        If set, will return tensors instead of list of python integers. Acceptable values are:\n",
      "\n",
      "        - `'tf'`: Return TensorFlow `tf.constant` objects.\n",
      "        - `'pt'`: Return PyTorch `torch.Tensor` objects.\n",
      "        - `'np'`: Return Numpy `np.ndarray` objects.\n",
      "\n",
      "    return_token_type_ids (`bool`, *optional*):\n",
      "        Whether to return token type IDs. If left to the default, will return the token type IDs according to\n",
      "        the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are token type IDs?](../glossary#token-type-ids)\n",
      "    return_attention_mask (`bool`, *optional*):\n",
      "        Whether to return the attention mask. If left to the default, will return the attention mask according\n",
      "        to the specific tokenizer's default, defined by the `return_outputs` attribute.\n",
      "\n",
      "        [What are attention masks?](../glossary#attention-mask)\n",
      "    return_overflowing_tokens (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return overflowing token sequences. If a pair of sequences of input ids (or a batch\n",
      "        of pairs) is provided with `truncation_strategy = longest_first` or `True`, an error is raised instead\n",
      "        of returning overflowing tokens.\n",
      "    return_special_tokens_mask (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return special tokens mask information.\n",
      "    return_offsets_mapping (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return `(char_start, char_end)` for each token.\n",
      "\n",
      "        This is only available on fast tokenizers inheriting from [`PreTrainedTokenizerFast`], if using\n",
      "        Python's tokenizer, this method will raise `NotImplementedError`.\n",
      "    return_length  (`bool`, *optional*, defaults to `False`):\n",
      "        Whether or not to return the lengths of the encoded inputs.\n",
      "    verbose (`bool`, *optional*, defaults to `True`):\n",
      "        Whether or not to print more information and warnings.\n",
      "    **kwargs: passed to the `self.tokenize()` method\n",
      "\n",
      "Return:\n",
      "    [`BatchEncoding`]: A [`BatchEncoding`] with the following fields:\n",
      "\n",
      "    - **input_ids** -- List of token ids to be fed to a model.\n",
      "\n",
      "      [What are input IDs?](../glossary#input-ids)\n",
      "\n",
      "    - **token_type_ids** -- List of token type ids to be fed to a model (when `return_token_type_ids=True` or\n",
      "      if *\"token_type_ids\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are token type IDs?](../glossary#token-type-ids)\n",
      "\n",
      "    - **attention_mask** -- List of indices specifying which tokens should be attended to by the model (when\n",
      "      `return_attention_mask=True` or if *\"attention_mask\"* is in `self.model_input_names`).\n",
      "\n",
      "      [What are attention masks?](../glossary#attention-mask)\n",
      "\n",
      "    - **overflowing_tokens** -- List of overflowing tokens sequences (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **num_truncated_tokens** -- Number of tokens truncated (when a `max_length` is specified and\n",
      "      `return_overflowing_tokens=True`).\n",
      "    - **special_tokens_mask** -- List of 0s and 1s, with 1 specifying added special tokens and 0 specifying\n",
      "      regular sequence tokens (when `add_special_tokens=True` and `return_special_tokens_mask=True`).\n",
      "    - **length** -- The length of the inputs (when `return_length=True`)"
     ]
    }
   ],
   "source": [
    "sentiment_tokenizer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07759a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

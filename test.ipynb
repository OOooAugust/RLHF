{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "679972cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "result = subprocess.run('bash -c \"source /etc/network_turbo && env | grep proxy\"', shell=True, capture_output=True, text=True)\n",
    "output = result.stdout\n",
    "for line in output.splitlines():\n",
    "    if '=' in line:\n",
    "        var, value = line.split('=', 1)\n",
    "        os.environ[var] = value\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "03edcfdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "SFT_model = AutoModelForCausalLM.from_pretrained(\"./qwen_imdb_final\")\n",
    "SFT_tokenizer = AutoTokenizer.from_pretrained(\"./qwen_imdb_final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dbe1e8d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset_test = load_dataset(\"stanfordnlp/imdb\", split=\"test\")\n",
    "\n",
    "def prompt_completion_preprocess(example):\n",
    "    words = example['text'].split()\n",
    "    prompt = ' '.join(words[:5])\n",
    "    completion = ' '.join(words[5:])\n",
    "    return {'prompt': prompt, 'completion': completion}\n",
    "\n",
    "dataset_test = dataset_test.map(prompt_completion_preprocess, remove_columns=['text', 'label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "068f41e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['prompt', 'completion'],\n",
       "    num_rows: 25000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c9ffc9b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting `pad_token_id` to `eos_token_id`:151643 for open-end generation.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A: I love sci-fi and am always happy to see a good movie every once in a while. I was in a bad mood, because I had to drive to Chicago, and this movie just happened to be up on TV.<br /><br />I will not give away any plot details here, except that the movie is set in 2022 and revolves around a nuclear-powered car. Unfortunately, the car gets into a terrible accident that shuts it off and makes it possible for a very small group of people to get on board in the hope that it will be repaired. However, some people died before it was fixed, and so everyone looks like a casualty of the accident. The accident makes the humans' face shorter and bulvier, so in order to walk normally, they have to use their necks and arms like a bunch of gymnasts, and in order to climb the rocks, their legs have to kick with all their strength (something that the non-mechanical humans don't have to worry about). <br /><br />You know that this is going to end up being bad right away, but I expected much more when I rented the movie. Instead, I sat there in the movie theater, frowning all the time, thinking that something was going to happen with the characters that is totally unexpected and tragic, but nothing ever happened. It has been said that if you watch the beginning and end of a movie, you can predict exactly what will happen in-between. This movie ended up being nothing at all, except that the characters died horribly and were repaired horribly in the end.\n",
      "B: I love sci-fi and am obsessed with the original book. I've read the books at least a hundred times and watched several episodes of the series as well as the first movie. I loved Robby the Robot in his original form, as I remember in the mid to late 1950's.<br /><br />I had been told by my doctor that if I was over 40 that I should get screened for any nuclear radiation at work as this was the biggest problem at the time. This meant that as a female and I was not allowed to work for a long time but this didn't stop me taking part in Sci-fi as I was into Robby the Robot and all those forms of nuclear energy - which included the most amazing 1950's science fiction animation movie of them all.<br /><br />I couldn't understand why it was that I couldn't see the movie properly but when my husband took me to see it on the big screen years ago we were appalled at the way the film looked in the cinema - in 1957! The big screen is not perfect and it would have looked even worse in my memory of that 1957 viewing. It is now a beautifully restored movie of the 1950's and should be seen by everyone as it is a great film in and of itself and still shows the wonder and imagination of the period.<br /><br />I recently went back to watch it just to confirm the restored version and the amazing animation is the same and so is the sound and it just doesn't compare to the 1957 viewing.<br /><br />My only regret is not seeing this film when it was released as it would have been a great Sci-fi film with Robby at his best - to the amazing shape he was in when the movie ended.<br /><br />See it and laugh you silly!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 1Ô∏è‚É£ Move model\n",
    "SFT_model = SFT_model.to(device)\n",
    "\n",
    "# 2Ô∏è‚É£ Tokenize and move inputs\n",
    "prefix = dataset_test[0]['prompt']\n",
    "inputs = SFT_tokenizer(prefix, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "\n",
    "# Ensure pad_token_id is set\n",
    "if SFT_tokenizer.pad_token_id is None:\n",
    "    SFT_tokenizer.pad_token = SFT_tokenizer.eos_token\n",
    "    SFT_model.config.pad_token_id = SFT_tokenizer.eos_token_id\n",
    "\n",
    "# Now generate, passing the full inputs\n",
    "outputs = SFT_model.generate(\n",
    "    **inputs,\n",
    "    num_return_sequences=2,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    temperature=1.0,\n",
    "    pad_token_id=SFT_model.config.pad_token_id,\n",
    ")\n",
    "\n",
    "completions = []\n",
    "for seq in outputs:\n",
    "    # skip the prompt tokens when decoding\n",
    "    text = SFT_tokenizer.decode(seq, \n",
    "                                skip_special_tokens=True)\n",
    "    completions.append(text)\n",
    "\n",
    "print(\"A:\", completions[0])\n",
    "print(\"B:\", completions[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7f6143b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForSequenceClassification(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 1024, padding_idx=1)\n",
       "      (position_embeddings): Embedding(514, 1024, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 1024)\n",
       "      (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-23): 24 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSdpaSelfAttention(\n",
       "              (query): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (key): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (value): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "              (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=1024, out_features=4096, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=4096, out_features=1024, bias=True)\n",
       "            (LayerNorm): LayerNorm((1024,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (classifier): RobertaClassificationHead(\n",
       "    (dense): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (out_proj): Linear(in_features=1024, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "sentiment_model = \"siebert/sentiment-roberta-large-english\"\n",
    "# 2Ô∏è‚É£ Load its tokenizer\n",
    "sentiment_tokenizer = AutoTokenizer.from_pretrained(sentiment_model)\n",
    "\n",
    "# 3Ô∏è‚É£ Load the model itself\n",
    "sentiment_model = AutoModelForSequenceClassification.from_pretrained(sentiment_model)\n",
    "\n",
    "# 4Ô∏è‚É£ (Optional) Move to GPU if available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "sentiment_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "81013463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('positive', [0.0011193063110113144, 0.9988806843757629])\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "def get_sentiment(text: str):\n",
    "    # Tokenize and prepare inputs\n",
    "    inputs = sentiment_tokenizer(text, return_tensors=\"pt\", truncation=True, padding=True)\n",
    "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
    "    # Forward pass\n",
    "    with torch.no_grad():\n",
    "        logits = sentiment_model(**inputs).logits\n",
    "    # Convert to probabilities\n",
    "    probs = F.softmax(logits, dim=-1)[0]\n",
    "    # Decode prediction\n",
    "    label = \"positive\" if probs[1] > probs[0] else \"negative\"\n",
    "    return label, probs.cpu().tolist()\n",
    "\n",
    "# üîç Example:\n",
    "print(get_sentiment(completions[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7a3e711c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"I love sci-fi and am always happy to see a good movie every once in a while. I was in a bad mood, because I had to drive to Chicago, and this movie just happened to be up on TV.<br /><br />I will not give away any plot details here, except that the movie is set in 2022 and revolves around a nuclear-powered car. Unfortunately, the car gets into a terrible accident that shuts it off and makes it possible for a very small group of people to get on board in the hope that it will be repaired. However, some people died before it was fixed, and so everyone looks like a casualty of the accident. The accident makes the humans' face shorter and bulvier, so in order to walk normally, they have to use their necks and arms like a bunch of gymnasts, and in order to climb the rocks, their legs have to kick with all their strength (something that the non-mechanical humans don't have to worry about). <br /><br />You know that this is going to end up being bad right away, but I expected much more when I rented the movie. Instead, I sat there in the movie theater, frowning all the time, thinking that something was going to happen with the characters that is totally unexpected and tragic, but nothing ever happened. It has been said that if you watch the beginning and end of a movie, you can predict exactly what will happen in-between. This movie ended up being nothing at all, except that the characters died horribly and were repaired horribly in the end.\",\n",
       " \"I love sci-fi and am obsessed with the original book. I've read the books at least a hundred times and watched several episodes of the series as well as the first movie. I loved Robby the Robot in his original form, as I remember in the mid to late 1950's.<br /><br />I had been told by my doctor that if I was over 40 that I should get screened for any nuclear radiation at work as this was the biggest problem at the time. This meant that as a female and I was not allowed to work for a long time but this didn't stop me taking part in Sci-fi as I was into Robby the Robot and all those forms of nuclear energy - which included the most amazing 1950's science fiction animation movie of them all.<br /><br />I couldn't understand why it was that I couldn't see the movie properly but when my husband took me to see it on the big screen years ago we were appalled at the way the film looked in the cinema - in 1957! The big screen is not perfect and it would have looked even worse in my memory of that 1957 viewing. It is now a beautifully restored movie of the 1950's and should be seen by everyone as it is a great film in and of itself and still shows the wonder and imagination of the period.<br /><br />I recently went back to watch it just to confirm the restored version and the amazing animation is the same and so is the sound and it just doesn't compare to the 1957 viewing.<br /><br />My only regret is not seeing this film when it was released as it would have been a great Sci-fi film with Robby at his best - to the amazing shape he was in when the movie ended.<br /><br />See it and laugh you silly!\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "completions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07759a44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
